name: medgemma-27b  # If lora on last 6 chars will be used in wandb name
server_image: registry.datexis.com/jfrick/vllm_server:nightly
client_image: registry.datexis.com/jfrick/vllm_client:latest
namespace: jfrick
load_from_checkpoint: False
Hardware:
  client_gpu: "p100"
  server_gpu: "h200"
  parallel_size: 2
Model:
#  base_model: "meta-llama/Meta-Llama-3.3-70B"
  base_model: "google/medgemma-27b-it"
#  base_model: "Qwen/Qwen3-32B"
  max_model_len: 8192
  max_num_seqs: 512
  max_num_batched_tokens: 32768  # 131072 32768 65536 131072
  dtype: "auto"
  thinking: False
  lora: False
  lora_modules: "qwen3-8b-sft-all-vall/checkpoint-1608"
  max_lora_rank: 128
Client_Job:
  num_choices: 30 # Set to 1 if guided decoding is applied
  num_samples: 3055
  concurrency: 8
  batch_size: 1
  start_verifier: 1
  budget: [4, 4, 4, 4]
  temperatures: [0.4, 0.4, 0.4, 0.4] # Set to 0 if guided decoding is applied
  max_tokens: [1500, 1300, 1500, 1800]
  thresholds: [0.4, 0.5, 0.8, 0.5]
  eval_mode: False
  ood_eval: False
  merlin_mode: True
  think_about_labs: True
  guided_decoding: False
  store_patients: True
